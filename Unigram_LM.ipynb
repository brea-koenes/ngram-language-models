{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c3701f-8127-48ab-a4cf-1142745145f2",
   "metadata": {
    "id": "51c3701f-8127-48ab-a4cf-1142745145f2"
   },
   "source": [
    "# Building a Unigram Language Model in NLP\n",
    "\n",
    "## By Brea Koenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785effb-1c39-4eb2-8d62-40032396feec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:12:56.277536Z",
     "iopub.status.busy": "2023-11-05T19:12:56.276538Z",
     "iopub.status.idle": "2023-11-05T19:12:56.292547Z",
     "shell.execute_reply": "2023-11-05T19:12:56.290549Z",
     "shell.execute_reply.started": "2023-11-05T19:12:56.277536Z"
    },
    "id": "3785effb-1c39-4eb2-8d62-40032396feec"
   },
   "source": [
    "### Overview\n",
    "\n",
    "Objective: Develop a program that calculates the probability of each word within a provided text corpus using a unigram model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- 1 - Import the Data\n",
    "\n",
    "- 2 - Data Cleaning and Tokenization\n",
    "\n",
    "- 3 - Probability Calculation\n",
    "\n",
    "- 4 - Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5280786-c750-4850-a379-ebf1fab0f22f",
   "metadata": {
    "id": "f5280786-c750-4850-a379-ebf1fab0f22f"
   },
   "source": [
    "### 01 - Importing the Data\n",
    "\n",
    "Data: Bram Stoker's classic novel \"Dracula\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577d0242-fb43-4968-a447-7e22db6f888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = 'pg345.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42bbb40-409d-42cf-b7ec-a45fd055543d",
   "metadata": {
    "id": "e42bbb40-409d-42cf-b7ec-a45fd055543d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List where each element is a line from the book\n",
    "dracula = []\n",
    "with open (data) as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip('\\n')\n",
    "        dracula.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfb879-112a-42e2-bea5-6a34267c9455",
   "metadata": {
    "id": "0bbfb879-112a-42e2-bea5-6a34267c9455"
   },
   "source": [
    "### 02 - Data Cleaning\n",
    "\n",
    "Refine the list `dracula` by converting all the text to lowercase, removing punctuation, and eliminating 'stop words'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086ee754-6383-49bb-8dd6-90b53638ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bkoenes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/bkoenes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bkoenes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffthe',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'dracula',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'united']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Takes list of text as input and returns a list of cleaned tokens.\n",
    "def cleanText(texts):\n",
    "    # Initialize\n",
    "    punctuation_list = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Add specific characters to remove\n",
    "    additional_chars_to_remove = [\"“\",\"’\",\"--\",\"”\"]\n",
    "    punctuation_list.extend(additional_chars_to_remove)\n",
    "    \n",
    "    cleaned_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Clean the tokens \n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            token = token.lower()\n",
    "            \n",
    "            if token in punctuation_list or token in stop_words:\n",
    "                continue\n",
    "                \n",
    "            cleaned_tokens.append(token)\n",
    "            \n",
    "        cleaned_texts.extend(cleaned_tokens)\n",
    "        \n",
    "    return cleaned_texts\n",
    "\n",
    "# Apply to data\n",
    "dracula = cleanText(dracula)\n",
    "dracula[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d4e12-058e-4c32-b8ca-3262f24a6800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T20:25:01.529008Z",
     "iopub.status.busy": "2023-11-05T20:25:01.528008Z",
     "iopub.status.idle": "2023-11-05T20:25:01.537019Z",
     "shell.execute_reply": "2023-11-05T20:25:01.536014Z",
     "shell.execute_reply.started": "2023-11-05T20:25:01.529008Z"
    },
    "id": "731d4e12-058e-4c32-b8ca-3262f24a6800"
   },
   "source": [
    "### 3 - Probability Calculation:\n",
    "\n",
    "Calculate the probability of occurrence for each word within our text. This is achieved by dividing the frequency of each word by the total number of words present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37f74aa-6e68-48d2-86b8-acfbde93e41c",
   "metadata": {
    "id": "f37f74aa-6e68-48d2-86b8-acfbde93e41c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "# Generate sets with 1 as the maximum size of n-grams and dracula as the data\n",
    "train_data, padded_vocab = padded_everygram_pipeline(1, dracula)\n",
    "\n",
    "# Transform padded_vocab into a list\n",
    "padded_vocab = list(padded_vocab)\n",
    "\n",
    "# Create an MLE unigram model\n",
    "unigram_model = MLE(1)\n",
    "\n",
    "# Fit the model using train_data and padded_vocab\n",
    "unigram_model.fit(train_data, padded_vocab)\n",
    "\n",
    "# Construct a dictionary of unigram probabilities\n",
    "unigram_probs = {word: unigram_model.score(word) for word in padded_vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b1204-a9ef-42e9-984c-3c78f6cb51a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T21:07:54.089930Z",
     "iopub.status.busy": "2023-11-05T21:07:54.088936Z",
     "iopub.status.idle": "2023-11-05T21:07:54.103450Z",
     "shell.execute_reply": "2023-11-05T21:07:54.102497Z",
     "shell.execute_reply.started": "2023-11-05T21:07:54.089930Z"
    },
    "id": "a42b1204-a9ef-42e9-984c-3c78f6cb51a9"
   },
   "source": [
    "### 4 - Presentation:\n",
    "\n",
    "Display the top 10 most probable words along with their corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2639c59c-1e50-4749-a685-d5540f3c7a16",
   "metadata": {
    "id": "2639c59c-1e50-4749-a685-d5540f3c7a16",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:0.12772979339451343\n",
      "a:0.07334004024144869\n",
      "t:0.0697305785935123\n",
      "s:0.06914658683810178\n",
      "o:0.06852578887961917\n",
      "n:0.06785100848996417\n",
      "r:0.06663149629484222\n",
      "i:0.06148108161162095\n",
      "l:0.05518967463316484\n",
      "d:0.049894488884526675\n"
     ]
    }
   ],
   "source": [
    "top_10_probs = sorted(unigram_probs.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_probs:\n",
    "    print(f\"{word}:{prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ec316-0cf0-446c-82f5-813ef5e8bbfc",
   "metadata": {
    "id": "b91ec316-0cf0-446c-82f5-813ef5e8bbfc"
   },
   "source": [
    "### 5 - Export Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f354947-dcfe-4522-b97a-409c12e2a0a6",
   "metadata": {
    "id": "7f354947-dcfe-4522-b97a-409c12e2a0a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the models in disk\n",
    "with open('unigram_model.pkl', 'wb') as file:\n",
    "    pickle.dump(unigram_model , file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
