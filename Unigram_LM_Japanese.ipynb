{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c3701f-8127-48ab-a4cf-1142745145f2",
   "metadata": {
    "id": "51c3701f-8127-48ab-a4cf-1142745145f2"
   },
   "source": [
    "# Building a Unigram Language Model in NLP - Japanese\n",
    "\n",
    "## By Brea Koenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785effb-1c39-4eb2-8d62-40032396feec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:12:56.277536Z",
     "iopub.status.busy": "2023-11-05T19:12:56.276538Z",
     "iopub.status.idle": "2023-11-05T19:12:56.292547Z",
     "shell.execute_reply": "2023-11-05T19:12:56.290549Z",
     "shell.execute_reply.started": "2023-11-05T19:12:56.277536Z"
    },
    "id": "3785effb-1c39-4eb2-8d62-40032396feec"
   },
   "source": [
    "### Overview\n",
    "\n",
    "\n",
    "Develop a program that calculates the probability of each word within a provided japanese text corpus using a unigram model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- 1 - Importing the Data\n",
    "\n",
    "- 2 - Data Cleaning and Tokenization: Preprocess the text by removing special characters and converting everything to lowercase to ensure uniformity. Split the text into individual words (unigrams) to analyze their frequencies.\n",
    "\n",
    "- 3 - Probability Calculation: Determine the probability of each word by dividing its frequency by the total number of words in the text.\n",
    "\n",
    "- 4 - Presentation: Display the top 10 most probable words along with their corresponding probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772842b-2f62-4428-b184-8d32ded07a22",
   "metadata": {
    "id": "5772842b-2f62-4428-b184-8d32ded07a22"
   },
   "source": [
    "### 01 - Importing the Data\n",
    "\n",
    "Data: Phillips Oppenheim's novel \"入れかわった男\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42bbb40-409d-42cf-b7ec-a45fd055543d",
   "metadata": {
    "id": "e42bbb40-409d-42cf-b7ec-a45fd055543d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "data = 'pg34158.txt'\n",
    "\n",
    "# Each element is a line from the book\n",
    "oppenheim = []\n",
    "with open (data) as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip('\\n')\n",
    "        oppenheim.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a62f0d-6fc1-444b-a878-727c5b87e9dd",
   "metadata": {
    "id": "a9a62f0d-6fc1-444b-a878-727c5b87e9dd"
   },
   "source": [
    "### 02 - Data Cleaning\n",
    "\n",
    "In traditional Japanese writing, spaces are not used to separate words. Japanese text is typically composed of continuous characters with no spaces between them. I add a space between every Japanese idiogram.\n",
    "\n",
    "Create a function that takes a list of text as input and returns a list of cleaned tokens. Each token in this list should be stripped of punctuation, any english, and free of ['stop words'](https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47).\n",
    "\n",
    "- For the 'stop words' the list is: ['の', 'に', 'は', 'を', 'た', 'が', 'で', 'て', 'と', 'し', 'れ', 'さ', 'ある', 'いる', 'も', 'する', 'から', 'な', 'こと', 'として', 'い', 'や', 'する', 'など', 'なり', 'なく', 'まで', 'だ', 'へ', 'か', 'だっ', 'その', 'あっ', 'よう', 'また', 'もの', 'という', 'あり', 'まし', 'ませ', 'う', 'ない', 'ながら', 'なけれ', 'なし', 'ず', 'なっ', 'れる', 'られ', 'なる', 'べき', 'ほど', 'ます', 'てる', 'なら', 'せる', 'され', 'して']\n",
    "\n",
    "- For the punctuation the list is: ['。', '、', '？', '！', '「', '」', '『', '』', '（', '）', '；', '：', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c518a9e-e0c4-4d2b-9c06-b9e95f75e205",
   "metadata": {
    "id": "7c518a9e-e0c4-4d2b-9c06-b9e95f75e205",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def is_japanese(word):\n",
    "    \"\"\"\n",
    "    Determine if a word is written in Japanese script.\n",
    "\n",
    "    This function checks if the given word contains any Japanese characters (Hiragana, Katakana, or Kanji)\n",
    "    and does not contain any Latin alphabet characters. If the word contains Japanese characters and no\n",
    "    Latin characters, it is considered a Japanese word.\n",
    "\n",
    "    Parameters:\n",
    "    word (str): The word to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the word is Japanese, False otherwise.\n",
    "    \"\"\"\n",
    "    # Regex to identify if the word contains any Japanese character\n",
    "    # Hiragana: U+3040-U+309F, Katakana: U+30A0-U+30FF, Kanji: U+4E00-U+9FAF\n",
    "    jap_regex = r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]'\n",
    "\n",
    "    # Regex to identify if the word contains any Latin alphabet character\n",
    "    eng_regex = r'[a-zA-Z]'\n",
    "\n",
    "    # If the word contains any Japanese characters and no Latin characters, it's considered Japanese\n",
    "    if re.search(jap_regex, word) and not re.search(eng_regex, word):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d27fcb1c-3102-4e65-ad5e-c6017398c711",
   "metadata": {
    "id": "d27fcb1c-3102-4e65-ad5e-c6017398c711",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['そ', 'ん', '長', 'ド', 'ミ', 'ニ', 'ー', 'つ', 'ぶ', '総', '督', '何'],\n",
       " ['植',\n",
       "  '民',\n",
       "  '地',\n",
       "  '領',\n",
       "  '陸',\n",
       "  '軍',\n",
       "  '司',\n",
       "  '令',\n",
       "  '官',\n",
       "  'す',\n",
       "  '特',\n",
       "  '別',\n",
       "  '任',\n",
       "  '務',\n",
       "  '受',\n",
       "  'け',\n",
       "  'こ',\n",
       "  '地',\n",
       "  'お',\n",
       "  'ら',\n",
       "  'る',\n",
       "  'す']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import string\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Takes list of text as input and returns a list of cleaned tokens.\n",
    "def cleanText(texts):\n",
    "    # Initialize\n",
    "    punctuation_list = {'。', '、', '？', '！', '「', '」', '『', '』', '（', '）', '；', '：', '-'}\n",
    "    stop_words = {'の', 'に', 'は', 'を', 'た', 'が', 'で', 'て', 'と', 'し', 'れ', 'さ', 'ある', 'いる', 'も', 'する', 'から', 'な', 'こと', 'として', 'い', 'や', 'する', 'など', 'なり', 'なく', 'まで', 'だ', 'へ', 'か', 'だっ', 'その', 'あっ', 'よう', 'また', 'もの', 'という', 'あり', 'まし', 'ませ', 'う', 'ない', 'ながら', 'なけれ', 'なし', 'ず', 'なっ', 'れる', 'られ', 'なる', 'べき', 'ほど', 'ます', 'てる', 'なら', 'せる', 'され', 'して'}\n",
    "    \n",
    "    cleaned_texts = []\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    for text in texts:\n",
    "        \n",
    "        # Remove punctuation first\n",
    "        text = ''.join([char if char not in punctuation_list else ' ' for char in text])\n",
    "\n",
    "        # Tokenize text and remove non-japanese\n",
    "        tokens = [token.surface for token in tokenizer.tokenize(text) if is_japanese(token.surface)]\n",
    "\n",
    "        # Split each token into individual characters\n",
    "        char_list = [char for token in tokens for char in token]\n",
    "\n",
    "        # Remove stop words\n",
    "        filtered_chars = [char for char in char_list if char not in stop_words]\n",
    "\n",
    "        # Ensure this specific character is removed\n",
    "        filtered_chars = [char for char in filtered_chars if char != '々']\n",
    "\n",
    "        cleaned_texts.append(filtered_chars)\n",
    "\n",
    "    return cleaned_texts\n",
    "\n",
    "oppenheim = cleanText(oppenheim)\n",
    "oppenheim[81:83]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d4e12-058e-4c32-b8ca-3262f24a6800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T20:25:01.529008Z",
     "iopub.status.busy": "2023-11-05T20:25:01.528008Z",
     "iopub.status.idle": "2023-11-05T20:25:01.537019Z",
     "shell.execute_reply": "2023-11-05T20:25:01.536014Z",
     "shell.execute_reply.started": "2023-11-05T20:25:01.529008Z"
    },
    "id": "731d4e12-058e-4c32-b8ca-3262f24a6800"
   },
   "source": [
    "### 3 - Probability Calculation:\n",
    "\n",
    "Calculate the probability of occurrence for each word within the text. This is achieved by dividing the frequency of each word by the total number of words present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f37f74aa-6e68-48d2-86b8-acfbde93e41c",
   "metadata": {
    "id": "f37f74aa-6e68-48d2-86b8-acfbde93e41c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "# Generate sets with 1 as the maximum size of n-grams and dracula as the data\n",
    "train_data, padded_vocab = padded_everygram_pipeline(1, oppenheim)\n",
    "\n",
    "# Transform padded_vocab into a list\n",
    "padded_vocab = list(padded_vocab)\n",
    "\n",
    "# Create an MLE unigram model\n",
    "unigram_model = MLE(1)\n",
    "\n",
    "# Fit the model using train_data and padded_vocab\n",
    "unigram_model.fit(train_data, padded_vocab)\n",
    "\n",
    "# Construct a dictionary of unigram probabilities\n",
    "unigram_probs = {word: unigram_model.score(word) for word in padded_vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be60261-4f7b-4368-a131-113684f895c2",
   "metadata": {
    "id": "5be60261-4f7b-4368-a131-113684f895c2"
   },
   "source": [
    "### 4 - Presentation:\n",
    "\n",
    "Display the top 10 most probable words along with their corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2639c59c-1e50-4749-a685-d5540f3c7a16",
   "metadata": {
    "id": "2639c59c-1e50-4749-a685-d5540f3c7a16",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "っ:0.03913621814856383\n",
      "る:0.02861952861952862\n",
      "ら:0.025501932909340316\n",
      "こ:0.024036662925551816\n",
      "ー:0.019131645674855553\n",
      "ま:0.018404206675811614\n",
      "あ:0.017915783347882113\n",
      "す:0.017915783347882113\n",
      "り:0.017271480234443196\n",
      "わ:0.017177952363137548\n"
     ]
    }
   ],
   "source": [
    "top_10_probs = sorted(unigram_probs.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_probs:\n",
    "    print(f\"{word}:{prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ec316-0cf0-446c-82f5-813ef5e8bbfc",
   "metadata": {
    "id": "b91ec316-0cf0-446c-82f5-813ef5e8bbfc"
   },
   "source": [
    "### 5 - Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f354947-dcfe-4522-b97a-409c12e2a0a6",
   "metadata": {
    "id": "7f354947-dcfe-4522-b97a-409c12e2a0a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the models in disk\n",
    "with open('unigram_model_japanese.pkl', 'wb') as file:\n",
    "    pickle.dump(unigram_model , file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
