{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee881faf-a1d3-4f46-b59b-ed4d80f503ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T00:35:00.552864Z",
     "iopub.status.busy": "2023-11-07T00:35:00.552864Z",
     "iopub.status.idle": "2023-11-07T00:35:00.576403Z",
     "shell.execute_reply": "2023-11-07T00:35:00.573396Z",
     "shell.execute_reply.started": "2023-11-07T00:35:00.552864Z"
    },
    "id": "ee881faf-a1d3-4f46-b59b-ed4d80f503ac"
   },
   "source": [
    "# N-gram Text Correction using Natural Language Processing\n",
    "\n",
    "## By Brea Koenes\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "There is a physical manuscript, possibly by William Shakespeare, with parts lost due to time and poor storage. Using Natural Language Processing, deploy n-gram and fivegram models to predict and weave in the missing words, aiming to restore the manuscript.\n",
    "\n",
    "Datasets:  \n",
    "\n",
    "- WS_train.txt - All WS works.\n",
    "\n",
    "- WS_test.txt - The manuscript, with the words lost marked as `<DELETED>`.\n",
    "    \n",
    "- WS_validation - Text to validade models performance.\n",
    "    \n",
    "    \n",
    "**Objective**: Implement an N-gram language model that can predict missing words in a text corpus, using the works of William Shakespeare as a basis for model training and evaluation.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess the training and test texts, preparing them for the N-gram model training.\n",
    "2. **N-gram Model Training - parts A and B**: Utilize the `WS_train.txt` to create an N-gram language model that learns the word sequences and their probabilities.\n",
    "3. **Text Correction**: Apply the trained N-gram model to predict and fill in the `<DELETED>` placeholders in `WS_test.txt`.\n",
    "4. **Evaluation**: Compare the corrected text against `WS_validation.txt` to evaluate the accuracy of the N-gram model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142010f-2d7f-4f0c-bbae-6a9a6a571306",
   "metadata": {
    "id": "3142010f-2d7f-4f0c-bbae-6a9a6a571306"
   },
   "source": [
    "### 1 - Data Preparation\n",
    "\n",
    "Define specific functions for loading, cleaning, generating N-grams, and building a vocabulary from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38694b6-2ee9-4412-b9b5-698de512c961",
   "metadata": {
    "id": "b38694b6-2ee9-4412-b9b5-698de512c961"
   },
   "source": [
    "#### 1.1 - Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1c91b83-9441-461d-9829-429d6fac993c",
   "metadata": {
    "id": "b1c91b83-9441-461d-9829-429d6fac993c"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aad37c3-d98e-4bca-bd1c-743a66747661",
   "metadata": {
    "id": "4aad37c3-d98e-4bca-bd1c-743a66747661"
   },
   "source": [
    "#### 1.2 - Load the Text\n",
    "\n",
    "Create a function `load_text` that reads a text file and returns its contents as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d78d462-bcc1-4793-9e20-d292db6ab10d",
   "metadata": {
    "id": "5d78d462-bcc1-4793-9e20-d292db6ab10d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load file\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "train_text = load_text('WS_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b8da2-ac99-4c28-94a2-5dd086906358",
   "metadata": {
    "id": "445b8da2-ac99-4c28-94a2-5dd086906358"
   },
   "source": [
    "#### 1.3 - Clean the Text\n",
    "\n",
    "Define a function named `clean_text` to standardize, tokenize, and remove punctuation from the text data, while retaining the **<DELETED>** placeholders. \n",
    "\n",
    "The function will:\n",
    "    \n",
    "1. Convert the text to lowercase.\n",
    "2. Tokenize\n",
    "3. Remove all punctuation tokens using `string.punctuation`.\n",
    "4. Remove stop words tokens using NLTK's `stopwords.words('english')`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e18d19b4-af11-4d47-8a7f-362cbd7a6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c4b3f09-4885-48cc-92d0-6f2ccee29263",
   "metadata": {
    "id": "4c4b3f09-4885-48cc-92d0-6f2ccee29263"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609',\n",
       " 'sonnets',\n",
       " 'william',\n",
       " 'shakespeare',\n",
       " '1',\n",
       " 'fairest',\n",
       " 'creatures',\n",
       " 'desire',\n",
       " 'increase',\n",
       " 'thereby',\n",
       " 'beauty',\n",
       " \"'s\",\n",
       " 'rose',\n",
       " 'might',\n",
       " 'never',\n",
       " 'die',\n",
       " 'riper',\n",
       " 'time',\n",
       " 'decease',\n",
       " 'tender',\n",
       " 'heir',\n",
       " 'might',\n",
       " 'bear',\n",
       " 'memory',\n",
       " 'thou',\n",
       " 'contracted',\n",
       " 'thine',\n",
       " 'bright',\n",
       " 'eyes',\n",
       " \"feed'st\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes list of text as input and returns a list of cleaned tokens.\n",
    "def clean_text(texts):\n",
    "    \n",
    "    # Lowercase\n",
    "    texts = texts.lower()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(texts)\n",
    "    \n",
    "    # Initialize\n",
    "    punctuation_list = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == \"deleted\":\n",
    "            cleaned_tokens.append(token) # Keep if it's <deleted>\n",
    "        elif token not in punctuation_list and token not in stop_words: \n",
    "            cleaned_tokens.append(token) # Otherwise also keep if not punctuation or stop word \n",
    "\n",
    "    # Replace tokenized strings in list to return it to original <DELETED>\n",
    "    old_substring = \"deleted\"  \n",
    "    new_substring = \"<DELETED>\"  \n",
    " \n",
    "    cleaned_tokens = [s.replace(old_substring, new_substring) for s in cleaned_tokens]  \n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Apply to list\n",
    "cleaned_train_text = clean_text(train_text)\n",
    "cleaned_train_text[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddfb7e-1427-4ccc-bd73-9cb751aa6bd3",
   "metadata": {
    "id": "a1ddfb7e-1427-4ccc-bd73-9cb751aa6bd3"
   },
   "source": [
    "#### 1.4 - Create N-grams\n",
    "\n",
    "Define a function `create_ngrams` that will convert a list of tokens into a list of N-grams. The function should take the following parameters:\n",
    "\n",
    "- `tokens`: A list of words (tokens) from which to create N-gram\n",
    "\n",
    "- `n`: The order of the N-gram (e.g., 2 for bigrams, 3 for trigrams, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5384bd91-01b3-40bf-b709-c2c57d7d817b",
   "metadata": {
    "id": "5384bd91-01b3-40bf-b709-c2c57d7d817b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', '<s>', '<s>', '1609'),\n",
       " ('<s>', '<s>', '<s>', '1609', 'sonnets'),\n",
       " ('<s>', '<s>', '1609', 'sonnets', 'william'),\n",
       " ('<s>', '1609', 'sonnets', 'william', 'shakespeare'),\n",
       " ('1609', 'sonnets', 'william', 'shakespeare', '1'),\n",
       " ('sonnets', 'william', 'shakespeare', '1', 'fairest'),\n",
       " ('william', 'shakespeare', '1', 'fairest', 'creatures'),\n",
       " ('shakespeare', '1', 'fairest', 'creatures', 'desire'),\n",
       " ('1', 'fairest', 'creatures', 'desire', 'increase'),\n",
       " ('fairest', 'creatures', 'desire', 'increase', 'thereby'),\n",
       " ('creatures', 'desire', 'increase', 'thereby', 'beauty'),\n",
       " ('desire', 'increase', 'thereby', 'beauty', \"'s\"),\n",
       " ('increase', 'thereby', 'beauty', \"'s\", 'rose'),\n",
       " ('thereby', 'beauty', \"'s\", 'rose', 'might'),\n",
       " ('beauty', \"'s\", 'rose', 'might', 'never')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def create_ngrams(tokens, n):\n",
    "    \n",
    "    modified_tokens = ['<s>'] * (n-1) + tokens +['</s>']\n",
    "\n",
    "    n_grams = list(ngrams(modified_tokens,n))\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "# Apply to data\n",
    "train_bigrams = create_ngrams(cleaned_train_text, 2)\n",
    "train_fivegrams = create_ngrams(cleaned_train_text, 5)\n",
    "train_fivegrams[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f78e348e-3a77-45e5-80f7-4151b6d2190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distributed', 'used', 'commercially', 'prohibited', 'commercial'),\n",
       " ('used', 'commercially', 'prohibited', 'commercial', 'distribution'),\n",
       " ('commercially', 'prohibited', 'commercial', 'distribution', 'includes'),\n",
       " ('prohibited', 'commercial', 'distribution', 'includes', 'service'),\n",
       " ('commercial', 'distribution', 'includes', 'service', 'charges'),\n",
       " ('distribution', 'includes', 'service', 'charges', 'download'),\n",
       " ('includes', 'service', 'charges', 'download', 'time'),\n",
       " ('service', 'charges', 'download', 'time', 'membership.'),\n",
       " ('charges', 'download', 'time', 'membership.', 'end'),\n",
       " ('download', 'time', 'membership.', 'end', 'etext'),\n",
       " ('time', 'membership.', 'end', 'etext', 'complete'),\n",
       " ('membership.', 'end', 'etext', 'complete', 'works'),\n",
       " ('end', 'etext', 'complete', 'works', 'william'),\n",
       " ('etext', 'complete', 'works', 'william', 'shakespeare'),\n",
       " ('complete', 'works', 'william', 'shakespeare', '</s>')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fivegrams[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443fb75-76b7-41eb-b52c-b97ecdc45133",
   "metadata": {
    "id": "0443fb75-76b7-41eb-b52c-b97ecdc45133"
   },
   "source": [
    "#### 1.5 - Build Vocabulary:\n",
    "\n",
    "Define a function `build_vocab` that creates a set of unique words from a list of tokens, EXCLUDING the `<DELETED>` placeholder. The function takes the following parameter:\n",
    "\n",
    "- `tokens`: A **list** of clean tokens from which to build the vocabulary.\n",
    "    \n",
    "The function `build_vocab` should return a `set` of unique tokens, which is the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa46fe22-0ca0-46af-bf10-6a6cfe8185a2",
   "metadata": {
    "id": "aa46fe22-0ca0-46af-bf10-6a6cfe8185a2"
   },
   "outputs": [],
   "source": [
    "# Create vocab set\n",
    "def build_vocab(tokens):\n",
    "    vocab_set = {token for token in tokens if token != '<DELETED>'}\n",
    "    return vocab_set\n",
    "\n",
    "# Apply to data\n",
    "vocab = build_vocab(cleaned_train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a78bd-1d45-4c22-badf-284dbd2ebb30",
   "metadata": {
    "id": "b31a78bd-1d45-4c22-badf-284dbd2ebb30"
   },
   "source": [
    "### 2A - N-gram Model Training - part A\n",
    "\n",
    "Training the N-gram model. This process involves calculating the frequency distribution of N-grams and estimating their probabilities based on the training corpus. These statistics will then be used to predict the next word in a sequence or to determine the most likely correction in a text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cd41e-b194-4440-bbc6-1513e660f728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T20:00:12.890464Z",
     "iopub.status.busy": "2024-03-01T20:00:12.889463Z",
     "iopub.status.idle": "2024-03-01T20:00:12.911201Z",
     "shell.execute_reply": "2024-03-01T20:00:12.910189Z",
     "shell.execute_reply.started": "2024-03-01T20:00:12.890464Z"
    },
    "id": "bb4cd41e-b194-4440-bbc6-1513e660f728"
   },
   "source": [
    "#### 2A.1 - Import necessary libraries:\n",
    "\n",
    ">```python\n",
    "from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "008b76bc-1503-4bb6-8b2d-47641ae3601b",
   "metadata": {
    "id": "008b76bc-1503-4bb6-8b2d-47641ae3601b"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397be122-e6eb-4332-90d3-987e01b94bf5",
   "metadata": {
    "id": "397be122-e6eb-4332-90d3-987e01b94bf5"
   },
   "source": [
    "#### 2A.2 - Calculate Frequency Distribution\n",
    "\n",
    "Create a function `calculate_ngram_freq` to calculate the frequency distribution of N-grams in the training data, employing the `FreqDist` class from the `nltk` library. The function should take the following parameter:\n",
    "\n",
    "- `ngrams_list`: A list of N-grams for which to calculate the frequency distribution.\n",
    "\n",
    "The function should return:\n",
    "\n",
    "- A `FreqDist` object representing the Frequency Distribution of the input N-grams.\n",
    "\n",
    "Using the function `calculate_ngram_freq`, create two variables named `bigram_freq_dist` and `fivegram_freq_dist`. Use the appropriate N-grams list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff06292b-4cc7-4752-a29b-53795d8902d3",
   "metadata": {
    "id": "ff06292b-4cc7-4752-a29b-53795d8902d3"
   },
   "outputs": [],
   "source": [
    "def calculate_ngram_freq(ngrams_list):\n",
    "    freq_dist = FreqDist(tuple(ngram) for ngram in ngrams_list)\n",
    "    return freq_dist\n",
    "\n",
    "# Apply to data\n",
    "bigram_freq_dist = calculate_ngram_freq(train_bigrams)\n",
    "fivegram_freq_dist = calculate_ngram_freq(train_fivegrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549782d-8d5d-4826-b0eb-d097c870049b",
   "metadata": {
    "id": "5549782d-8d5d-4826-b0eb-d097c870049b"
   },
   "source": [
    "#### 2A.3 - Probability Estimation\n",
    "\n",
    "Create a function `estimate_ngram_probabilities` to estimate the conditional probabilities of N-grams, utilizing the `ConditionalFreqDist` and `ConditionalProbDist` classes along with a probability distribution such as `MLEProbDist` from the `nltk.probability` module. The function should take the following parameter:\n",
    "    \n",
    "- `ngrams_list`: A list of N-grams for which to estimate conditional probabilities.\n",
    "   \n",
    "The function should return:\n",
    "\n",
    "- A `ConditionalProbDist` object representing the conditional probabilities of the input N-grams.\n",
    "\n",
    "\n",
    "Using the function `Probability Estimation `, create two variables named `bigram_prob_dist` and `fivegram_prob_dist`. Use the appropriate N-grams list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccdb45bd-a2f5-41b6-96d1-60e3d2904fe7",
   "metadata": {
    "id": "ccdb45bd-a2f5-41b6-96d1-60e3d2904fe7"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "\n",
    "def estimate_ngram_probabilities(ngrams_list):\n",
    "    cfdist = ConditionalFreqDist()\n",
    "    for ngram in ngrams_list:\n",
    "        condition = tuple(ngram[:-1])\n",
    "        word = ngram[-1]\n",
    "        cfdist[condition][word] += 1\n",
    "    \n",
    "    return ConditionalProbDist(cfdist, MLEProbDist)\n",
    "\n",
    "# Estimate probabilities\n",
    "bigram_prob_dist = estimate_ngram_probabilities(train_bigrams)\n",
    "fivegram_prob_dist = estimate_ngram_probabilities(train_fivegrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07086b93-8715-467d-8dd8-d0ffa00b8664",
   "metadata": {
    "id": "07086b93-8715-467d-8dd8-d0ffa00b8664"
   },
   "source": [
    "### 2B - N-gram Model Training - part B\n",
    "\n",
    "Create the core function of the ngram NLP: the function `predict_next_word`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f337c-7f13-4fe9-883f-eb50f25b4653",
   "metadata": {
    "id": "d87f337c-7f13-4fe9-883f-eb50f25b4653"
   },
   "source": [
    "#### 2A.1 - Import necessary libraries:\n",
    "\n",
    ">```python\n",
    "from nltk.probability import ConditionalProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9c249b2-aaa4-4f01-96c6-8f6b1fc76fa6",
   "metadata": {
    "id": "e9c249b2-aaa4-4f01-96c6-8f6b1fc76fa6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalProbDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c0230-7792-4173-b8fa-9677c9448f60",
   "metadata": {
    "id": "005c0230-7792-4173-b8fa-9677c9448f60"
   },
   "source": [
    "#### 2A.2 - Predict next word\n",
    "\n",
    "Create a function `predict_next_word` that utilizes the conditional probabilities to predict the most probable next word after a given context.\n",
    "\n",
    "The function should take the following parameters:\n",
    "\n",
    "- A context, which is a tuple of words that precedes the word to be predicted. The size of the context should be N-1 for an N-gram model.\n",
    "- A `ConditionalProbDist` object that has been previously computed from the training data.\n",
    "- Optionally accepts an integer `top_n` that specifies the number of top probable next words to return (default is 1, which returns the most probable next word).\n",
    "\n",
    "\n",
    "The function should handle cases where the context is not found in the `ConditionalProbDist`, returning the default value `<UNK>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be315d11-9383-45a9-8940-4985ab70b371",
   "metadata": {
    "id": "be315d11-9383-45a9-8940-4985ab70b371",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict next word function\n",
    "def predict_next_word(context, prob_dist, top_n=1):\n",
    "    if context in prob_dist:\n",
    "        return prob_dist[context].max() if top_n == 1 else prob_dist[context].samples()[:top_n]\n",
    "    return '<UNK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd363f-9d3b-45df-bd46-c0c7dc47bb54",
   "metadata": {
    "id": "e8dd363f-9d3b-45df-bd46-c0c7dc47bb54"
   },
   "source": [
    "### 3 - Text Correction\n",
    "\n",
    "After training our N-gram model, the next step is to apply it to correct texts that contain placeholders indicating missing words. In this section, import the test text and use our model to predict the words that should replace the `<DELETED>` placeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281db13-98ac-407c-8c19-36beb1b7b631",
   "metadata": {
    "id": "7281db13-98ac-407c-8c19-36beb1b7b631"
   },
   "source": [
    "#### 3.1 - Correction Function\n",
    "\n",
    "Create a function `correct_text_with_ngrams` that searches for `<DELETED>` placeholders in the test data and uses the `predict_next_word` function to find the most probable replacement.\n",
    "\n",
    "The function should take the following parameters:\n",
    "\n",
    "- `text_data`: The list of tokens from the test data, including `<DELETED>` placeholders.\n",
    "- `ngram_model`: The trained N-gram model to use for prediction (e.g., bigram or fivegram model).\n",
    "- `n`: The order of the N-gram (e.g., 2 for bigrams, 3 for trigrams, etc.).\n",
    "\n",
    "The function should return:\n",
    "\n",
    "- `corrected_text`: A **list** of tokens where `<DELETED>` placeholders have been replaced with the most probable word predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65bb845d-a034-4b6d-aeb1-7cac45125409",
   "metadata": {
    "id": "65bb845d-a034-4b6d-aeb1-7cac45125409",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text correction function\n",
    "def correct_text_with_ngrams(text_data, ngram_model, n):\n",
    "    corrected_text = []\n",
    "    for i in range(len(text_data)):\n",
    "        if text_data[i] == '<DELETED>':\n",
    "            context = tuple(corrected_text[-(n-1):])\n",
    "            predicted_word = predict_next_word(context, ngram_model)\n",
    "            corrected_text.append(predicted_word)\n",
    "        else:\n",
    "            corrected_text.append(text_data[i])\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d567c1-0c23-4f85-9012-b8b954fe5a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-01T20:32:23.975490Z",
     "iopub.status.busy": "2024-03-01T20:32:23.974493Z",
     "iopub.status.idle": "2024-03-01T20:32:23.995597Z",
     "shell.execute_reply": "2024-03-01T20:32:23.994651Z",
     "shell.execute_reply.started": "2024-03-01T20:32:23.975490Z"
    },
    "id": "17d567c1-0c23-4f85-9012-b8b954fe5a5f"
   },
   "source": [
    "#### 3.2 - Load and Clean Test Data\n",
    "\n",
    "Import the `WS_test.txt` file using the `load_text` function. Then, apply the `clean_text` function to prepare the data for correction.\n",
    "\n",
    "Execute the `load_text` function to import the content of `WS_test.txt` and store it in a variable named `test_text`.\n",
    "Apply `clean_text` to `test_text` to obtain a tokenized and cleaned list of words, including `<DELETED>` placeholders, and store it in a variable named `cleaned_test_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86487111-476f-4c90-97b5-f7729f8987d3",
   "metadata": {
    "id": "86487111-476f-4c90-97b5-f7729f8987d3"
   },
   "outputs": [],
   "source": [
    "# Read in and apply clean_text\n",
    "test_text = load_text('WS_test.txt')\n",
    "cleaned_test_text = clean_text(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d986ba-bb8c-4a00-9a49-3a2d89543c41",
   "metadata": {
    "id": "75d986ba-bb8c-4a00-9a49-3a2d89543c41"
   },
   "source": [
    "#### 3.3 Applying Bigram Model\n",
    "\n",
    "Apply the `correct_text_with_ngrams` function to the `cleaned_test_text` using the bigram `bigram_prob_dist` object created before. Save the output to a variable named `corrected_test_text_bigram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "835f26a3-1189-4f40-9e76-217b59c241ba",
   "metadata": {
    "id": "835f26a3-1189-4f40-9e76-217b59c241ba"
   },
   "outputs": [],
   "source": [
    "corrected_test_text_bigram = correct_text_with_ngrams(cleaned_test_text, bigram_prob_dist, n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6f978-6c90-47da-8f2d-69c44409d23d",
   "metadata": {
    "id": "99c6f978-6c90-47da-8f2d-69c44409d23d"
   },
   "source": [
    "#### 3.4 Applying Fivegram Model\n",
    "\n",
    "Apply the `correct_text_with_ngrams` function to the `cleaned_test_text` using the bigram `fivegram_prob_dist` object created before. Save the output to a variable named `corrected_test_text_fivegram`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "812f6820-16e6-4197-a0aa-9836ea6d8fd2",
   "metadata": {
    "id": "812f6820-16e6-4197-a0aa-9836ea6d8fd2"
   },
   "outputs": [],
   "source": [
    "corrected_test_text_fivegram = correct_text_with_ngrams(cleaned_test_text, fivegram_prob_dist, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b782631-daa6-46f9-94bf-502584650e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T02:33:01.664693Z",
     "iopub.status.busy": "2023-11-07T02:33:01.664693Z",
     "iopub.status.idle": "2023-11-07T02:33:01.675715Z",
     "shell.execute_reply": "2023-11-07T02:33:01.674704Z",
     "shell.execute_reply.started": "2023-11-07T02:33:01.664693Z"
    },
    "id": "0b782631-daa6-46f9-94bf-502584650e01",
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluate the performance of our N-gram model. Calculate the accuracy of our model by comparing the predicted text against a validation text that contains the correct words.\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "1. **Import and Clean Validation Data**: Load and preprocess the `WS_validation.txt` file to obtain a clean list of tokens for accuracy comparison.\n",
    "\n",
    "2. **Accuracy Calculation**: Use the supplied function `calculate_accuracy` to calculate the accurary for both models.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "1. **Load Validation Text**: Use the `load_text` function to import the `WS_validation.txt` file.\n",
    "\n",
    "2. **Clean Validation Text**: Apply the `clean_text` function to the imported validation text to produce a list of clean tokens for comparison.\n",
    "\n",
    "3. **Accuracy Calculation**: This function takes:\n",
    "\n",
    "   - `test_tokens`: A list of tokes with the `<DELETED>` placeholder (before correction).\n",
    "   - `corrected_tokens`: A list of tokens that have been corrected by the N-gram model (either bigram or fivegram).\n",
    "   - `validation_tokens`: A list of clean tokens from the validation text.<B></B>\n",
    "   \n",
    "The function should return the accuracy as a float, calculated as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Import and Clean Validation Data**:\n",
    "\n",
    "   - Use the `load_text` function to load the `WS_validation.txt` as the validation text data. Store it as `validation_text`.\n",
    "   - Apply the `clean_text` function to the loaded `validation_text` to obtain a list of tokens for accuracy comparison, named `cleaned_validation_text`.<B></B>\n",
    "\n",
    "2. **Accuracy Calculation**:\n",
    "\n",
    "   - Store the resulting accuracy scores in variables named `bigram_accuracy` and `fivegram_accuracy`.<B></B>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3f960b1-d697-45cb-b72e-99b9b25f50fa",
   "metadata": {
    "id": "b3f960b1-d697-45cb-b72e-99b9b25f50fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(test_tokens, corrected_tokens, validation_tokens):\n",
    "    if (len(test_tokens) != len(corrected_tokens)) or (len(test_tokens) != len(validation_tokens)):\n",
    "        print(\"Test Tokens, Validation Token and Corrected Tokens must have the same length\")\n",
    "        return\n",
    "\n",
    "    correct_predictions = 0\n",
    "    all_predictions = 0\n",
    "    for i in range(len(test_tokens)):\n",
    "        if test_tokens[i] == '<DELETED>':\n",
    "            all_predictions+=1\n",
    "            if corrected_tokens[i] == validation_tokens[i]:\n",
    "                   correct_predictions+=1\n",
    "\n",
    "    return correct_predictions/all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29f1aad5-4020-400e-91bc-00f53f4adfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and clean text\n",
    "validation_text = load_text('WS_validation.txt')\n",
    "cleaned_validation_text = clean_text(validation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e12d73f8-ac83-42ef-aa98-ef2adba99c6f",
   "metadata": {
    "id": "e12d73f8-ac83-42ef-aa98-ef2adba99c6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "bigram_accuracy = calculate_accuracy(cleaned_test_text, corrected_test_text_bigram, cleaned_validation_text)\n",
    "fivegram_accuracy = calculate_accuracy(cleaned_test_text, corrected_test_text_fivegram, cleaned_validation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e846d-a59e-4fff-8786-4be6239f26c3",
   "metadata": {
    "id": "9e2e846d-a59e-4fff-8786-4be6239f26c3"
   },
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "474ae79c-9265-42df-ae07-02363f042162",
   "metadata": {
    "id": "474ae79c-9265-42df-ae07-02363f042162",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('fivegram_prob_dist.pkl', 'wb') as file:\n",
    "    pickle.dump(fivegram_prob_dist , file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
